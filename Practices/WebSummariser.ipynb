{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3e6c74d-f0e7-4639-8b5c-c44ea39cdb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5e041b9-6744-4685-91a1-0df366113aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8f59bde-6294-4316-9c81-00ae0dd3deaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_website_content(url):\n",
    "    \"\"\"\n",
    "    Fetch the textual content of a website, ignoring irrelevant elements.\n",
    "    :param url: The URL of the website to summarize.\n",
    "    :return: Cleaned and relevant text content of the website.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Ensure request was successful\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Remove irrelevant elements like <script>, <style>, etc.\n",
    "        for element in soup([\"script\", \"style\", \"header\", \"footer\", \"nav\", \"aside\"]):\n",
    "            element.decompose()\n",
    "\n",
    "        # Extract relevant text from <p>, <h1>, <h2>, <h3>, <li>, etc.\n",
    "        text_elements = soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li'])\n",
    "        text = ' '.join([elem.get_text(strip=True) for elem in text_elements])\n",
    "\n",
    "        # Optional: Remove excessive whitespace\n",
    "        cleaned_text = ' '.join(text.split())\n",
    "        return cleaned_text\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the website content: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30b4a8ba-c3d8-4161-a4f0-053d80f024c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(text, max_length=2000):\n",
    "    \"\"\"\n",
    "    Splits the text into smaller chunks, ensuring each chunk is within the maximum length.\n",
    "    :param text: The input text to split.\n",
    "    :param max_length: The maximum length of each chunk.\n",
    "    :return: A list of text chunks.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for word in words:\n",
    "        if len(\" \".join(current_chunk + [word])) <= max_length:\n",
    "            current_chunk.append(word)\n",
    "        else:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "\n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a233f1f4-8f1e-4591-b857-5a93c2758692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text):\n",
    "    \"\"\"\n",
    "    Send text to the Ollama API to summarize, handling streaming responses.\n",
    "    :param text: The text to summarize.\n",
    "    :return: The summary returned by the Ollama API.\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"Summarize the following text.\"},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ]\n",
    "    }\n",
    "    try:\n",
    "        # Enable streaming by setting stream=True\n",
    "        response = requests.post(OLLAMA_API, headers=HEADERS, json=payload, stream=True)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Combine the streamed chunks into a single message\n",
    "        full_message = \"\"\n",
    "        for line in response.iter_lines(decode_unicode=True):\n",
    "            if line:  # Process only non-empty lines\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    message_content = data.get(\"message\", {}).get(\"content\", \"\")\n",
    "                    full_message += message_content\n",
    "                except json.JSONDecodeError:\n",
    "                    print(\"Invalid JSON in response line:\", line)\n",
    "\n",
    "        return full_message.strip() if full_message else \"No summary returned.\"\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error communicating with the Ollama API: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1f68c75-e73c-4670-aba6-170747f2b03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_large_text(text, max_length=2000):\n",
    "    \"\"\"\n",
    "    Splits large text into smaller chunks and summarizes each chunk separately.\n",
    "    :param text: The large text to summarize.\n",
    "    :param max_length: The maximum length of each chunk.\n",
    "    :return: A combined summary of the entire text.\n",
    "    \"\"\"\n",
    "    chunks = split_text_into_chunks(text, max_length)\n",
    "    summaries = []\n",
    "\n",
    "    print(f\"Text split into {len(chunks)} chunks for summarization.\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        print(f\"Summarizing chunk {i+1} of {len(chunks)}...\")\n",
    "        summary = summarize_text(chunk)\n",
    "        summaries.append(summary)\n",
    "\n",
    "    # Combine all summaries into a final summary\n",
    "    final_summary = \" \".join(summaries)\n",
    "    return final_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ead8eb6-3ed6-4fb3-a25a-65944d13bc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def website_summarizer(url):\n",
    "    \"\"\"\n",
    "    Fetches content from a website and summarizes it.\n",
    "    :param url: The URL of the website to summarize.\n",
    "    :return: The summarized text.\n",
    "    \"\"\"\n",
    "    print(f\"Fetching content from: {url}\")\n",
    "    website_content = fetch_website_content(url)\n",
    "    if not website_content:\n",
    "        return \"Failed to fetch website content.\"\n",
    "    \n",
    "    print(\"Summarizing the content...\")\n",
    "    summary = summarize_large_text(website_content)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26f11acd-e9f6-44b2-abc2-f2d0814b9776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a website URL to summarize:  https://www.teachermagazine.com/sea_en/articles/a-student-diary-project-improving-literacy-skills-and-wellbeing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching content from: https://www.teachermagazine.com/sea_en/articles/a-student-diary-project-improving-literacy-skills-and-wellbeing\n",
      "Summarizing the content...\n",
      "Text split into 2 chunks for summarization.\n",
      "Summarizing chunk 1 of 2...\n",
      "Summarizing chunk 2 of 2...\n",
      "\n",
      "Summary:\n",
      "\n",
      "The Ar Ridha Al Salaam School, an environmentally friendly Islamic school in Indonesia, is implementing a student diary project to improve literacy skills and wellbeing. Due to the pandemic, children were limited to home environments, leading to boredom. To address this issue, the school introduced a diary writing activity where students write about their daily lives for one week at a time, sharing their thoughts and ideas in simple language. The program was highly successful, with almost all students, including those from Grade 1, participating and showing enthusiasm. The project has been shown to be an effective way to improve literacy skills while also providing students with a platform to express themselves and connect with others during a challenging time. Here is a summary of the text:\n",
      "\n",
      "The author published two collections of student diaries in 2021, titled \"Learning Diary in a Pandemic Period\" (Grades 1-3) and \"I Miss School\" (Grades 4-6). The project aimed to develop literacy skills while also providing students with a positive experience during the pandemic. As part of this program, students are allowed to write about any topic they choose, which has had a positive impact on their engagement.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example URL\n",
    "    url = input(\"Enter a website URL to summarize: \")\n",
    "    summary = website_summarizer(url)\n",
    "    print(\"\\nSummary:\\n\")\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33880514-3127-4640-9953-60f6ad4e3846",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
